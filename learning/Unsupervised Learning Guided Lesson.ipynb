{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Unsupervised-Learning-Guided-Lesson\" data-toc-modified-id=\"Unsupervised-Learning-Guided-Lesson-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Unsupervised Learning Guided Lesson</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lesson-Goals\" data-toc-modified-id=\"Lesson-Goals-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Lesson Goals</a></span></li><li><span><a href=\"#Exploring-the-Variables\" data-toc-modified-id=\"Exploring-the-Variables-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Exploring the Variables</a></span></li><li><span><a href=\"#Some-More-Transformations---PCA\" data-toc-modified-id=\"Some-More-Transformations---PCA-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Some More Transformations - PCA</a></span></li><li><span><a href=\"#The-Algorithm---K-Means-Clustering\" data-toc-modified-id=\"The-Algorithm---K-Means-Clustering-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>The Algorithm - K-Means Clustering</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Guided Lesson\n",
    "\n",
    "\n",
    "## Lesson Goals\n",
    "\n",
    "In this guided lesson, we will analyze an unsupervised learning problem from start to finish and introduce several different processing techniques.\n",
    "\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "As a data scientist or analyst, you may be asked open ended question about a dataset. One example is to find some patterns in a dataset that is unlabeled. Typically, this happens when analyzing a group of customers and trying to find a common themes between the transactions. The leading choice of algorithm for this type of problem is an unsupervised algorithm. Specifically, this lesson will be using clustering to analyze this problem. In this lesson, we will be analyzing a log of transactions from a bakery to make recommendations about the marketing and sale of products.\n",
    "\n",
    "\n",
    "**The Data**\n",
    "\n",
    "The dataset we will be analyzing comes from Kaggle and is a log of transactions from a bakery in Edinburgh called The Bread Basket. We will start by evaluating the types of the variables in the data as well as the contents of the categorical variables and the distribution of the numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "breadbasket = pd.read_csv('../data/BreadBasket_DMS.csv')\n",
    "\n",
    "breadbasket.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breadbasket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breadbasket.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from these three functions, we have 4 columns in the dataset. The date and time are separated into two columns and stored as text. This means we will have to combine and convert them later on. The transaction variable is ordinal, so the summary statistics really have no meaning in this context. The only meaningful information from the describe function is the max which tells us we have 9684 transactions in the dataset. From the head function, we see that each row represents an item in the transaction. This means that there are potentially multiple items per transaction or maybe just one. We might benefit from consolidating the data and creating a new dataset that contains one row per transaction.\n",
    "\n",
    "So the data we have is a breakdown of each item in a transaction and the date and time when the transaction occurred. The data we do not have is any information about the customer. Since we cannot associate the transactions back to customers, we cannot tell if a certain customer is a regular who comes in and buys a coffee and a pastry every day or whether a customer is a tourist who came in once and bought a specialty dessert.\n",
    "\n",
    "Our strategy to make sense of the dataset will be to generate derived variables from this transaction log and cluster based on these derived variables. We will evaluate the aggregate information regarding each cluster and make recommendations about which products to advertise and which products should be in stock and at what days and times.\n",
    "\n",
    "\n",
    "## Exploring the Variables\n",
    "\n",
    "While we only have a few variables, we should explore their contents.\n",
    "\n",
    "First, let's look at the time and day of week. Hour is a crucial factor since customer behavior differs significantly between the morning and the afternoon. However, we can even find differences between customer behavior at 7 am vs. at 9 am. Similarly, we see differences between weekday and weekend customer behavior.\n",
    "\n",
    "To examine the date and time, we must reformat this variable. We start by combining the date and time into one column and then transforming the column to a datetime column. This allows us to extract the hour and the time of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breadbasket['DateTime'] = pd.to_datetime(breadbasket.Date + ' ' + breadbasket.Time)\n",
    "\n",
    "breadbasket.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at the item variable. This variable will tell us how many products are sold by the bakery and which products are more popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breadbasket.Item.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the counts as well to see what items are most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breadbasket.Item.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 95 items. The most common items are coffee, bread, tea, cake and pastry. Since 95 is a really large number, if we created dummy variables out of this data, it would produce too many variables. One option is to classify the data. Typically, when working on these types of problems, companies will have a classification system for the items they sell. However, let's try to come up with our own.\n",
    "\n",
    "Looking at the list of unique items, we can identify a number of obvious categories. We have beverages and breakfast pastries like muffins and medialuna. We have items for kids like juice and pouches. We also have non food items like gift vouchers and t-shirts. Another group of items that we can notice is ready to eat snacks like popcorn and crisps. With a bit of work, we can narrow it down from 95 products to 11 categories: beverage, other, kids, snacks, bread, breakfast pastry, dessert, condiments, breakfast, lunch, and other foods. The last group is used to classify mostly uncommon items that sell very little (like polenta) or have names that are hard to identify (like \"Hack the Stack\").\n",
    "\n",
    "We generate the categories using lists and then use the lists to create dummy variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beverage = ['Hot chocolate', 'Coffee', 'Tea', 'Mineral water', 'Juice', 'Coke', 'Smoothies']\n",
    "other = ['NONE', 'Christmas common', 'Gift voucher', \"Valentine's card\", 'Tshirt', 'Afternoon with the baker', 'Postcard', 'Siblings', 'Nomad bag', 'Adjustment', 'Drinking chocolate spoons ', 'Coffee granules ']\n",
    "kids = [\"Ella's Kitchen Pouches\", 'My-5 Fruit Shoot', 'Kids biscuit']\n",
    "snacks = ['Mighty Protein', 'Pick and Mix Bowls', 'Caramel bites', 'Bare Popcorn', 'Crisps', 'Cherry me Dried fruit', 'Raw bars']\n",
    "bread = ['Bread', 'Toast', 'Baguette', 'Focaccia', 'Scandinavian']\n",
    "breakfast_pastry = ['Muffin', 'Pastry', 'Medialuna', 'Scone']\n",
    "dessert = ['Cookies', 'Tartine', 'Fudge', 'Victorian Sponge', 'Cake', 'Alfajores', 'Brownie', 'Bread Pudding', 'Bakewell', 'Raspberry shortbread sandwich', 'Lemon and coconut', 'Crepes', 'Chocolates', 'Truffles', 'Panatone']\n",
    "condiments = ['Jam', 'Dulce de Leche', 'Honey', 'Gingerbread syrup', 'Extra Salami or Feta', 'Bacon', 'Spread', 'Chimichurri Oil']\n",
    "breakfast = ['Eggs', 'Frittata', 'Granola', 'Muesli', 'Duck egg', 'Brioche and salami']\n",
    "lunch = ['Soup', 'Sandwich', 'Chicken sand', 'Salad', 'Chicken Stew']\n",
    "\n",
    "other_food = [x for x in breadbasket.Item.unique() if x not in beverage \n",
    "                and x not in other and x not in kids and x not in snacks \n",
    "                and x not in bread and x not in breakfast_pastry \n",
    "                and x not in dessert and x not in condiments \n",
    "                and x not in breakfast and x not in lunch]\n",
    "\n",
    "breadbasket['beverage'] = np.where(breadbasket.Item.isin(beverage), 1, 0)\n",
    "breadbasket['other'] = np.where(breadbasket.Item.isin(other), 1, 0)\n",
    "breadbasket['kids'] = np.where(breadbasket.Item.isin(kids), 1, 0)\n",
    "breadbasket['snacks'] = np.where(breadbasket.Item.isin(snacks), 1, 0)\n",
    "breadbasket['bread'] = np.where(breadbasket.Item.isin(bread), 1, 0)\n",
    "breadbasket['breakfast_pastry'] = np.where(breadbasket.Item.isin(breakfast_pastry), 1, 0)\n",
    "breadbasket['dessert'] = np.where(breadbasket.Item.isin(dessert), 1, 0)\n",
    "breadbasket['condiments'] = np.where(breadbasket.Item.isin(condiments), 1, 0)\n",
    "breadbasket['breakfast'] = np.where(breadbasket.Item.isin(breakfast), 1, 0)\n",
    "breadbasket['lunch'] = np.where(breadbasket.Item.isin(lunch), 1, 0)\n",
    "breadbasket['other_food'] = np.where(breadbasket.Item.isin(other_food), 1, 0)\n",
    "breadbasket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing the Data**\n",
    "\n",
    "The first bit of work we will do to process the data is to aggregate by transaction. This will give us the count of each category per transaction. We will use the groupby function to find the sum in each transaction. We group by the datetime as well since we want to keep this column after the aggregation. This should not be a problem since a transaction number and a datetime uniquely identifies each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread_group = breadbasket.groupby(['Transaction', 'DateTime']).sum()\n",
    "\n",
    "bread_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the transaction number and the datetime are indices in this aggregated dataset. If we would like to use the information in these columns, we would have to reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread_group.reset_index(level= ['DateTime'], inplace = True)\n",
    "\n",
    "bread_group.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will generate a column for day of week and for hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread_group['hour'] = bread_group.DateTime.dt.hour\n",
    "bread_group['day'] = bread_group.DateTime.dt.day_name()\n",
    "\n",
    "bread_group.day.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saturday has the most transactions of any weekday by far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread_group.hour.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 am has the most transactions followed by noon and 10 am.\n",
    "\n",
    "Now let's create dummy variables out of the day column and drop all other non numeric columns to prepare our dataset for the ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some More Transformations - PCA\n",
    "\n",
    "Our plan here is to use k-means clustering. However, an important note on k-means clustering is that it does not respond well to dummy variable columns. Therefore, our best option is to transform the data using principal component analysis or PCA. What PCA does is project our data onto a lower dimensional subspace. The new data will typically reduce the dimensions of our original data and will therefore, contain less variables. The first dimension will explain the most amount of variation in the data and subsequent components will explain less and less variation. This transformation will provide us with a smaller amount of continuous variables that we can cluster more effectively.\n",
    "\n",
    "Here we chose to generate 4 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread_days = pd.get_dummies(data = bread_group, columns = ['day'])\n",
    "\n",
    "bread_days.dtypes\n",
    "\n",
    "bread_days.drop(columns = ['Item', 'DateTime', 'Date', 'Time'], inplace = True, axis = 1)\n",
    "\n",
    "bread_days.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 4)\n",
    "\n",
    "principalComponents = pca.fit_transform(bread_days)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2', 'pc3', 'pc4'])\n",
    "\n",
    "principalDf.head()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm - K-Means Clustering\n",
    "\n",
    "We are now ready to cluster the data using k-means. We chose to create 5 clusters. The choice is normally arbitrary though there are ways to optimize the number of clusters. Here, the choice is more driven by the number of transaction clusters we would like to create. Two clusters would definitely be too few to capture meaningful differences while 10 is certainly too many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 5)\n",
    "\n",
    "bread_clusters = kmeans.fit(principalDf)\n",
    "\n",
    "bread_clusters.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the labels back to our original data so we can do some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread_days['labels'] = bread_clusters.fit_predict(principalDf)\n",
    "\n",
    "bread_days.reset_index('Transaction', inplace = True)\n",
    "\n",
    "bread_merged = pd.merge(breadbasket, bread_days[['Transaction', 'labels']], on = 'Transaction', how = 'outer')\n",
    "\n",
    "bread_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's so some analysis on the clusters. First let's look at how many transactions we have per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread_merged.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest cluster is the 5th cluster (our clusters are numbered 0 through 4).\n",
    "\n",
    "One interesting thing to check is whether the clusters captured a different type of transaction by looking at the hour breakdown for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(bread_days.hour, bread_days.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see a separation. Clusters 0, 2, and 4 center around noon. Cluster 1 is an early morning cluster. Cluster 3 is an evening cluster.\n",
    "\n",
    "We can do the same analysis for day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(bread_group.day, bread_days.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cluster 1 (the early morning cluster), the disparity between the weekends and the weekdays is small. While in the clusters that center around later times, there seem to be more transactions during the weekends.\n",
    "\n",
    "Let's also look at the top 5 products per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = bread_merged.groupby(['labels']).Item.value_counts()\n",
    "\n",
    "b = a.to_frame(\"counts\").reset_index()\n",
    "\n",
    "b.set_index(\"Item\", inplace = True)\n",
    "\n",
    "b.groupby('labels').counts.nlargest(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While tea and coffee are popular in all 3 clusters, the morning cluster (Cluster 1) contains only bread, beverages and breakfast pastries. Clusters 0 and 2 are afternoon cluster and contain cake and sandwiches as top items. Cluster 3 is also an afternoon cluster and contains more desserts.\n",
    "\n",
    "We can use this data to run promotions for certain items like cake and sandwiches at certain hours to increase our sales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
